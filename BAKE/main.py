# main.py

import os
import argparse
import sys
import yaml  # [New] å‹™å¿…åŠ å…¥é€™è¡Œ
from core.llm_client import LLMClient
from core.bake_engine import BakeEngine
from utils import config_loader, data_loader

def parse_arguments():
    # ... (ä¿æŒåŸæ¨£)
    parser = argparse.ArgumentParser(description='BAKE Automation Runner')
    parser.add_argument('--scorer_model', type=str, help='Override scorer model name') # å¦‚æœæ‚¨å·²ç¶“æ”¹åç‚º eval_model è«‹å°æ‡‰ä¿®æ”¹
    parser.add_argument('--eval_model', type=str, help='Override evaluation (scorer) model name') # é…åˆ BAKE.sh
    parser.add_argument('--optimizer_model', type=str, help='Override optimizer model name')
    parser.add_argument('--opt_model', type=str, help='Override optimizer model name') # é…åˆ BAKE.sh
    
    parser.add_argument('--dataset_limit', type=int, help='Override dataset limit per subset')
    parser.add_argument('--limit', type=int, help='Override dataset limit') # é…åˆ BAKE.sh
    
    parser.add_argument('--output_dir', type=str, required=True, help='Directory to save all outputs')
    parser.add_argument('--iterative', action='store_true', help='Enable iterative prompt updates based on rules')
    parser.add_argument('--iterative_prompt_count', type=int, help='Number of prompts to generate in iterative mode')
    parser.add_argument('--iterative_count', type=int, help='Number of prompts') # é…åˆ BAKE.sh
    
    # Dataset ç›¸é—œ
    parser.add_argument('--task', type=str, choices=['mmlu', 'gsm8k'], help='Choose active dataset')
    parser.add_argument('--subsets', type=str, help='Comma-separated subsets')
    parser.add_argument('--split', type=str, help='Override dataset split')

    return parser.parse_args()

def main():
    args = parse_arguments()
    
    cfg = config_loader.load_config()
    meta_prompts = config_loader.load_meta_prompts(cfg['paths']['meta_prompt_dir'])
    
    # --- 1. è™•ç†åƒæ•¸è¦†è“‹ (CLI Override) ---
    # æ”¯æ´æ–°èˆŠåƒæ•¸åç¨±ï¼Œç¢ºä¿ç›¸å®¹æ€§
    eval_model = args.eval_model or args.scorer_model
    if eval_model:
        cfg['evaluation']['model_name'] = eval_model # æ³¨æ„: æ ¹æ“šæ‚¨çš„ config çµæ§‹å¯èƒ½æ˜¯ cfg['scorer'] æˆ– cfg['evaluation']

    opt_model = args.opt_model or args.optimizer_model
    if opt_model:
        cfg['optimizer']['model_name'] = opt_model

    # Dataset è¨­å®š
    if args.task:
        cfg['dataset']['active_task'] = args.task
    
    active_task = cfg['dataset'].get('active_task', 'mmlu') # Default fallback
    task_cfg = cfg['dataset'].get(active_task, {}) # å–å¾—è©² task çš„ dict

    limit = args.limit if args.limit is not None else args.dataset_limit
    if limit is not None:
        task_cfg['limit'] = limit
        
    if args.split:
        task_cfg['split'] = args.split
        
    if active_task == 'mmlu' and args.subsets:
        task_cfg['subsets'] = [s.strip() for s in args.subsets.split(',')]
        
    cfg['dataset'][active_task] = task_cfg # å¯«å›

    # è¿­ä»£è¨­å®š
    cfg['bake']['iterative'] = args.iterative
    iter_count = args.iterative_count or args.iterative_prompt_count
    if iter_count:
        cfg['bake']['iterative_prompt_count'] = iter_count

    # --- 2. ç›®éŒ„è¨­å®š ---
    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)

    # ==========================================
    # [New] å„²å­˜å¯¦é©—ç•¶ä¸‹çš„ Config å¿«ç…§
    # ==========================================
    config_snapshot_path = os.path.join(args.output_dir, "experiment_config.yaml")
    print(f"ğŸ’¾ Saving experiment config to: {config_snapshot_path}")
    with open(config_snapshot_path, 'w', encoding='utf-8') as f:
        # ä½¿ç”¨ yaml.dump å°‡æœ€çµ‚çš„ cfg ç‰©ä»¶å¯«å…¥æª”æ¡ˆ
        yaml.dump(cfg, f, allow_unicode=True, default_flow_style=False)
    # ==========================================
    
    # --- 3. è·¯å¾‘é‡å° ---
    # ç¢ºä¿æ‰€æœ‰ log éƒ½å­˜åˆ° output_dir
    for key in ['output_file', 'detailed_log', 'rules_log', 'cost_log', 'opt_status', 'trace_log', 'prompt_history', 'rule_evolution']:
        if key in cfg['paths']:
            filename = os.path.basename(cfg['paths'][key])
            cfg['paths'][key] = os.path.join(args.output_dir, filename)

    # --- 4. åˆå§‹åŒ–èˆ‡åŸ·è¡Œ ---
    # è«‹æ ¹æ“šæ‚¨æœ€æ–°çš„ config çµæ§‹èª¿æ•´ key (ä¾‹å¦‚ cfg['evaluation'] æˆ– cfg['scorer'])
    # å‡è¨­æ‚¨å·²ç¶“æ›´æ–°ç‚ºæ–°çµæ§‹ï¼š
    scorer_cfg = cfg.get('evaluation', cfg.get('scorer')) 
    scorer = LLMClient(scorer_cfg, role='scorer', pricing=scorer_cfg.get('pricing', {}))
    
    optimizer = LLMClient(cfg['optimizer'], role='optimizer', pricing=cfg['optimizer']['pricing'])
    
    # è¼‰å…¥è³‡æ–™ (ä½¿ç”¨ data_loader çš„æ–°å‡½å¼)
    dataset = data_loader.load_specific_dataset(active_task, task_cfg)
    
    engine = BakeEngine(scorer, optimizer, cfg, meta_prompts)
    print(f"ğŸš€ BAKE Engine Started with {len(dataset)} samples...")
    
    try:
        final_prompts, final_rule = engine.run(dataset, cfg['initial_prompts'])
        
        with open(cfg['paths']['output_file'], "w", encoding="utf-8") as f:
            f.write("\n".join(final_prompts))
            
        rule_path = os.path.join(args.output_dir, "final_rule.txt")
        with open(rule_path, "w", encoding="utf-8") as f:
            f.write(final_rule)
        
        scorer.save_cost_record(cfg['paths']['cost_log'])
        optimizer.save_cost_record(cfg['paths']['cost_log'])
        
        print(f"\nâœ… Experiment Success!")
        print(f"   Saved to: {args.output_dir}")

    except Exception as e:
        print(f"\nâŒ Experiment Failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()